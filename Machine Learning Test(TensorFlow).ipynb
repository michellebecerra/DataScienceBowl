{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Preprocess and export the data to file (Obsolete)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "100\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "200\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "300\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "400\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "500\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "600\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "700\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "800\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "900\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1000\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1100\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1200\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1300\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1400\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "1500\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n",
      "This is unlabeled data!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "#SLICE_COUNT = 20\n",
    "SLICE_COUNT = 10\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "\n",
    "\n",
    "def process_data(patient,labels_df,img_px_size=50, hm_slices=20, visualize=False):\n",
    "    \n",
    "    label = labels_df.get_value(patient, 'cancer')\n",
    "    path = data_dir + patient\n",
    "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(np.array(each_slice.pixel_array),(img_px_size,img_px_size)) for each_slice in slices]\n",
    "    \n",
    "    chunk_sizes = math.ceil(len(slices) / hm_slices)\n",
    "    for slice_chunk in chunks(slices, chunk_sizes):\n",
    "        slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(slice_chunk)\n",
    "\n",
    "    if len(new_slices) == hm_slices-1:\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == hm_slices-2:\n",
    "        new_slices.append(new_slices[-1])\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == hm_slices+2:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "        \n",
    "    if len(new_slices) == hm_slices+1:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "\n",
    "    if visualize:\n",
    "        fig = plt.figure()\n",
    "        for num,each_slice in enumerate(new_slices):\n",
    "            y = fig.add_subplot(4,5,num+1)\n",
    "            y.imshow(each_slice, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    if label == 1: label=np.array([0,1])\n",
    "    elif label == 0: label=np.array([1,0])\n",
    "        \n",
    "    return np.array(new_slices),label\n",
    "\n",
    "#                                               stage 1 for real.\n",
    "data_dir = 'D:/stage1/'\n",
    "patients = os.listdir(data_dir)\n",
    "labels = pd.read_csv('data/stage1_labels.csv', index_col=0)\n",
    "\n",
    "much_data = []\n",
    "for num,patient in enumerate(patients):\n",
    "    if num % 100 == 0:\n",
    "        print(num)\n",
    "    try:\n",
    "        img_data,label = process_data(patient,labels,img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT)\n",
    "        #print(img_data.shape,label)\n",
    "        much_data.append([img_data,label])\n",
    "    except KeyError as e:\n",
    "        print('This is unlabeled data!')\n",
    "\n",
    "np.save('muchdata-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,SLICE_COUNT), much_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Load preprocessed data, train and save model (Obsolete)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a0008585d1b0>:69: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 10 loss: 147567120837.0\n",
      "Accuracy: 0.7\n",
      "Epoch 2 completed out of 10 loss: 16274273583.0\n",
      "Accuracy: 0.7\n",
      "Epoch 3 completed out of 10 loss: 5269966279.02\n",
      "Accuracy: 0.6\n",
      "Epoch 4 completed out of 10 loss: 2688508502.5\n",
      "Accuracy: 0.5\n",
      "Epoch 5 completed out of 10 loss: 1715656093.11\n",
      "Accuracy: 0.6\n",
      "Epoch 6 completed out of 10 loss: 925859933.625\n",
      "Accuracy: 0.6\n",
      "Epoch 7 completed out of 10 loss: 572327315.552\n",
      "Accuracy: 0.5\n",
      "Epoch 8 completed out of 10 loss: 273143603.493\n",
      "Accuracy: 0.7\n",
      "Epoch 9 completed out of 10 loss: 124091884.593\n",
      "Accuracy: 0.5\n",
      "Epoch 10 completed out of 10 loss: 108039876.408\n",
      "Accuracy: 0.6\n",
      "Done. Finishing accuracy:\n",
      "Accuracy: 0.8\n",
      "fitment percent: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "SLICE_COUNT = 10\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "keep_rate = 0.8\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window as you slide about\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32]), name='w_conv1'),\n",
    "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64]), name='w_conv2'),\n",
    "               #                                  64 features\n",
    "               'W_fc':tf.Variable(tf.random_normal([32448,1024]), name='w_fc'), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(10/2/2) * 64\n",
    "               #'W_fc':tf.Variable(tf.random_normal([54080,1024]), name='W_fc'), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(20/2/2) * 64\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]), name='w_out')}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32]), name='b_conv1'),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64]), name='b_conv2'),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024]), name='b_fc'),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]), name='b_out')}\n",
    "\n",
    "    #                            image X      image Y        image Z\n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "\n",
    "\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,32448])\n",
    "    #fc = tf.reshape(conv2,[-1, 54080])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "\n",
    "    return output\n",
    "\n",
    "much_data = np.load('muchdata-50-50-10.npy')\n",
    "# If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\n",
    "train_data = much_data[:-100]\n",
    "validation_data = much_data[-10:]\n",
    "del much_data\n",
    "\n",
    "def train_neural_network(x):  \n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    X = data[0]\n",
    "                    Y = data[1]\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_runs += 1\n",
    "                except Exception as e:\n",
    "                    # I am passing for the sake of notebook space, but we are getting 1 shaping issue from one \n",
    "                    # input tensor. Not sure why, will have to look into it. Guessing it's\n",
    "                    # one of the depths that doesn't come to 20.\n",
    "                    pass\n",
    "                    #print(str(e))\n",
    "            \n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "            \n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'my-save-dir/my-model-10')\n",
    "\n",
    "# Run this locally:\n",
    "train_neural_network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Train and save model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/stage1/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a14012ca4f0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/stage1/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel_save_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'my-save-dir'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpatients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mlabel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/stage1_labels.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:/stage1/'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import time\n",
    "\n",
    "start_time = time.time() #start timer\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "SLICE_COUNT = 20\n",
    "#SLICE_COUNT = 10\n",
    "\n",
    "cache_dir = \"cache/\"\n",
    "data_dir = 'D:/stage1/'\n",
    "model_save_folder = 'my-save-dir'\n",
    "patients = os.listdir(data_dir)\n",
    "shuffle(patients)\n",
    "label_file = 'data/stage1_labels.csv'\n",
    "labels = pd.read_csv(label_file, index_col=0)\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 300\n",
    "training_size = 1000\n",
    "validation_size = 100\n",
    "\n",
    "keep_rate = 0.8\n",
    "\n",
    "def clear_cache():\n",
    "    folder = cache_dir[:-1]\n",
    "    if os.path.isdir(folder):\n",
    "        for the_file in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, the_file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "\n",
    "\n",
    "def process_data(patient,labels_df,img_px_size=50, hm_slices=20):\n",
    "    label = labels_df.get_value(patient, 'cancer')\n",
    "    path = data_dir + patient\n",
    "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(np.array(each_slice.pixel_array),(img_px_size,img_px_size)) for each_slice in slices]\n",
    "\n",
    "    chunk_sizes = math.ceil(len(slices) / hm_slices)\n",
    "    for slice_chunk in chunks(slices, chunk_sizes):\n",
    "        slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(slice_chunk)\n",
    "\n",
    "    while len(new_slices) < hm_slices:\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    while len(new_slices) > hm_slices:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "\n",
    "    if label == 1: label=np.array([0,1])\n",
    "    elif label == 0: label=np.array([1,0])\n",
    "\n",
    "    return np.array(new_slices),label\n",
    "\n",
    "def GetPatientsPreprocessedData(start, end):\n",
    "    cacheFilePath = cache_dir + str(start) + '-' + str(end) + '.npy'\n",
    "    if os.path.exists(cacheFilePath):\n",
    "        X, Y = np.load(cacheFilePath)\n",
    "        return list(X), list(Y)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        patient = patients[i]\n",
    "        try:\n",
    "            img_data,label = process_data(patient,labels,img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT)\n",
    "            #print(img_data.shape,label)\n",
    "            X.append(img_data)\n",
    "            Y.append(label)\n",
    "        except KeyError as e:\n",
    "            #print('This is unlabeled data!')\n",
    "            pass\n",
    "\n",
    "    np.save(cacheFilePath, [X, Y])\n",
    "    return [X, Y]\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window as you slide about\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #                                  64 features\n",
    "               #'W_fc':tf.Variable(tf.random_normal([32448,1024])), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(10/2/2) * 64\n",
    "               'W_fc':tf.Variable(tf.random_normal([54080,1024]), name='W_fc'), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(20/2/2) * 64\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    #                            image X      image Y        image Z\n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "\n",
    "\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    #fc = tf.reshape(conv2,[-1,32448])\n",
    "    fc = tf.reshape(conv2,[-1, 54080])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.add(tf.matmul(fc, weights['out']), biases['out'])\n",
    "\n",
    "    return output\n",
    "\n",
    "clear_cache()\n",
    "validation_data = GetPatientsPreprocessedData(len(patients) - validation_size, len(patients))\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, math.ceil(training_size / batch_size)): #train in batches\n",
    "                print('Batch', i+1)\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    X, Y = GetPatientsPreprocessedData(i * batch_size, min((i +\n",
    "                        1) * batch_size, training_size))\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_runs += 1\n",
    "                except Exception as e:\n",
    "                    # I am passing for the sake of notebook space, but we are getting 1 shaping issue from one\n",
    "                    # input tensor. Not sure why, will have to look into it. Guessing it's\n",
    "                    # one of the depths that doesn't come to 20.\n",
    "                    print(str(e))\n",
    "                    pass\n",
    "\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print('Accuracy:',accuracy.eval({x:[i for i in validation_data[0]], y:[i for i in validation_data[1]]}))\n",
    "\n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i for i in validation_data[0]], y:[i for i in validation_data[1]]}))\n",
    "\n",
    "        print('fitment percent:',successful_runs/total_runs)\n",
    "\n",
    "        tf.add_to_collection('input', x)\n",
    "        tf.add_to_collection('output', prediction)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if not os.path.isdir(model_save_folder):\n",
    "            os.makedirs(model_save_folder)\n",
    "\n",
    "        saver.save(sess, model_save_folder + '/my-model')\n",
    "\n",
    "# Run this locally:\n",
    "train_neural_network(x)\n",
    "durationS = time.time() - start_time\n",
    "durationH = int(durationS / 3600)\n",
    "durationS = durationS % 3600\n",
    "durationM = int(durationS / 60)\n",
    "durationS = durationS % 60\n",
    "print(\"--- %s hours %s minutes %s seconds ---\" % (durationH, durationM, durationS)) #print time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Same as above but this time there is additional pre-processing added</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 324 346 346\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dicom\n",
    "import os\n",
    "import scipy.ndimage\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection \n",
    "\n",
    "start_time = time.time() #start timer\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "SLICE_COUNT = 20\n",
    "\n",
    "cache_dir = \"cache/\"\n",
    "data_dir = 'D:/stage1/'\n",
    "model_save_folder = 'my-save-dir'\n",
    "patients = os.listdir(data_dir)\n",
    "shuffle(patients)\n",
    "label_file = 'data/stage1_labels.csv'\n",
    "labels = pd.read_csv(label_file, index_col=0)\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 300\n",
    "training_size = 1000\n",
    "validation_size = 100\n",
    "keep_rate = 0.8\n",
    "\n",
    "# Load the scans in given folder path\n",
    "def load_scan(path):\n",
    "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "        \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Set outside-of-scan pixels to 0\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    for slice_number in range(len(slices)):\n",
    "        \n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        \n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "            \n",
    "        image[slice_number] += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def resample(image, scan, new_spacing=[1,1,1]):\n",
    "    # Determine current pixel spacing\n",
    "    spacing = np.array([scan[0].SliceThickness] + scan[0].PixelSpacing, dtype=np.float32)\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest')\n",
    "    \n",
    "    return image, new_spacing\n",
    "\n",
    "def largest_label_volume(im, bg=-1):\n",
    "    vals, counts = np.unique(im, return_counts=True)\n",
    "\n",
    "    counts = counts[vals != bg]\n",
    "    vals = vals[vals != bg]\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        return vals[np.argmax(counts)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def segment_lung_mask(image, fill_lung_structures=True):\n",
    "    \n",
    "    # not actually binary, but 1 and 2. \n",
    "    # 0 is treated as background, which we do not want\n",
    "    binary_image = np.array(image > -320, dtype=np.int8)+1\n",
    "    labels = measure.label(binary_image)\n",
    "    \n",
    "    # Pick the pixel in the very corner to determine which label is air.\n",
    "    #   Improvement: Pick multiple background labels from around the patient\n",
    "    #   More resistant to \"trays\" on which the patient lays cutting the air \n",
    "    #   around the person in half\n",
    "    background_label = labels[0,0,0]\n",
    "    \n",
    "    #Fill the air around the person\n",
    "    binary_image[background_label == labels] = 2\n",
    "    \n",
    "    \n",
    "    # Method of filling the lung structures (that is superior to something like \n",
    "    # morphological closing)\n",
    "    if fill_lung_structures:\n",
    "        # For every slice we determine the largest solid structure\n",
    "        for i, axial_slice in enumerate(binary_image):\n",
    "            axial_slice = axial_slice - 1\n",
    "            labeling = measure.label(axial_slice)\n",
    "            l_max = largest_label_volume(labeling, bg=0)\n",
    "            \n",
    "            if l_max is not None: #This slice contains some lung\n",
    "                binary_image[i][labeling != l_max] = 1\n",
    "\n",
    "    \n",
    "    binary_image -= 1 #Make the image actual binary\n",
    "    binary_image = 1-binary_image # Invert it, lungs are now 1\n",
    "    \n",
    "    # Remove other air pockets insided body\n",
    "    labels = measure.label(binary_image, background=0)\n",
    "    l_max = largest_label_volume(labels, bg=0)\n",
    "    if l_max is not None: # There are air pockets\n",
    "        binary_image[labels != l_max] = 0\n",
    " \n",
    "    return binary_image\n",
    "    \n",
    "def normalize(image):\n",
    "    MIN_BOUND = -1000.0\n",
    "    MAX_BOUND = 400.0\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "def zero_center(image):\n",
    "    PIXEL_MEAN = 0.25\n",
    "    image = image - PIXEL_MEAN\n",
    "    return image\n",
    "\n",
    "def firstPreprocess(patient):\n",
    "    patient_img = load_scan(data_dir + patient)\n",
    "    patient_pixels = get_pixels_hu(patient_img)\n",
    "    pix_resampled, spacing = resample(patient_pixels, patient_img, [1,1,1])\n",
    "    #segmented_lungs = segment_lung_mask(pix_resampled, False)\n",
    "    segmented_lungs_fill = segment_lung_mask(pix_resampled, True)\n",
    "    finalOutput = np.multiply(segmented_lungs_fill, pix_resampled)\n",
    "    finalOutput = normalize(finalOutput)\n",
    "    finalOutput = zero_center(finalOutput)\n",
    "    return finalOutput\n",
    "\n",
    "def clear_cache():\n",
    "    folder = cache_dir[:-1]\n",
    "    if os.path.isdir(folder):\n",
    "        for the_file in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, the_file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "\n",
    "def secondpreprocess(slices, img_px_size, hm_slices):\n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(np.array(each_slice),(img_px_size,img_px_size)) for each_slice in slices]\n",
    "\n",
    "    chunk_sizes = math.ceil(len(slices) / hm_slices)\n",
    "    for slice_chunk in chunks(slices, chunk_sizes):\n",
    "        slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(slice_chunk)\n",
    "\n",
    "    while len(new_slices) < hm_slices:\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    while len(new_slices) > hm_slices:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "\n",
    "    return np.array(new_slices)\n",
    "\n",
    "def process_data(patient,labels_df,img_px_size=50, hm_slices=20):\n",
    "    data = secondpreprocess(firstPreprocess(patient), img_px_size, hm_slices)\n",
    "    label = labels_df.get_value(patient, 'cancer')\n",
    "\n",
    "    if label == 1: label=np.array([0,1])\n",
    "    elif label == 0: label=np.array([1,0])\n",
    "\n",
    "    return data, label\n",
    "\n",
    "def GetPatientsPreprocessedData(start, end):\n",
    "    cacheFilePath = cache_dir + str(start) + '-' + str(end) + '.npy'\n",
    "    if os.path.exists(cacheFilePath):\n",
    "        X, Y = np.load(cacheFilePath)\n",
    "        return list(X), list(Y)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        patient = patients[i]\n",
    "        try:\n",
    "            img_data,label = process_data(patient,labels,img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT)\n",
    "            #print(img_data.shape,label)\n",
    "            X.append(img_data)\n",
    "            Y.append(label)\n",
    "        except KeyError as e:\n",
    "            #print('This is unlabeled data!')\n",
    "            pass\n",
    "\n",
    "    np.save(cacheFilePath, [X, Y])\n",
    "    return [X, Y]\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window as you slide about\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #                                  64 features\n",
    "               #'W_fc':tf.Variable(tf.random_normal([32448,1024])), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(10/2/2) * 64\n",
    "               'W_fc':tf.Variable(tf.random_normal([54080,1024]), name='W_fc'), #54080 = ceil(50/2/2) * ceil(50/2/2) * ceil(20/2/2) * 64\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    #                            image X      image Y        image Z\n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "\n",
    "\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    #fc = tf.reshape(conv2,[-1,32448])\n",
    "    fc = tf.reshape(conv2,[-1, 54080])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.add(tf.matmul(fc, weights['out']), biases['out'])\n",
    "\n",
    "    return output\n",
    "\n",
    "clear_cache()\n",
    "validation_data = GetPatientsPreprocessedData(len(patients) - validation_size, len(patients))\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, math.ceil(training_size / batch_size)): #train in batches\n",
    "                print('Batch', i+1)\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    X, Y = GetPatientsPreprocessedData(i * batch_size, min((i +\n",
    "                        1) * batch_size, training_size))\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_runs += 1\n",
    "                except Exception as e:\n",
    "                    # I am passing for the sake of notebook space, but we are getting 1 shaping issue from one\n",
    "                    # input tensor. Not sure why, will have to look into it. Guessing it's\n",
    "                    # one of the depths that doesn't come to 20.\n",
    "                    print(str(e))\n",
    "                    pass\n",
    "\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print('Accuracy:',accuracy.eval({x:[i for i in validation_data[0]], y:[i for i in validation_data[1]]}))\n",
    "\n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i for i in validation_data[0]], y:[i for i in validation_data[1]]}))\n",
    "\n",
    "        print('fitment percent:',successful_runs/total_runs)\n",
    "\n",
    "        tf.add_to_collection('input', x)\n",
    "        tf.add_to_collection('output', prediction)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if not os.path.isdir(model_save_folder):\n",
    "            os.makedirs(model_save_folder)\n",
    "\n",
    "        saver.save(sess, model_save_folder + '/my-model')\n",
    "\n",
    "# Run this locally:\n",
    "train_neural_network(x)\n",
    "durationS = time.time() - start_time\n",
    "durationH = int(durationS / 3600)\n",
    "durationS = durationS % 3600\n",
    "durationM = int(durationS / 60)\n",
    "durationS = durationS % 60\n",
    "print(\"--- %s hours %s minutes %s seconds ---\" % (durationH, durationM, durationS)) #print time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Restore trained model and predict</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Initialized\n",
      "ac4056071f3cc98489b9db3aebfe2b6a\n",
      "ae2fdcd8daa3fede6ae23cc63a8d9a82\n",
      "ae4e9d8aab8f8f5ae975bcca923f468d\n",
      "ae61ec94b0b8de5439180f4776551e42\n",
      "aec5a58fea38b77b964007aa6975c049\n",
      "af1d0c2fcde369dd1b715460c2f704a2\n",
      "b0599ad2f33276e7cd065eaa8dcec8a2\n",
      "b17c07114dcf49ce71c8da4b43cf1192\n",
      "b4d5b618fdf3a5a1bcfb325a3715e99e\n",
      "b4db5b96c65a668a2e63f9a3ed36afe7\n",
      "b53d997901eb880c41fbfbc82847204c\n",
      "b6857d98b7b3dbe84f153617f4dfd14b\n",
      "b82efe72526c59a96257208d95e54baf\n",
      "b8793dbd40de88c0de0913abbaab0fe7\n",
      "bbf7a3e138f9353414f2d51f0c363561\n",
      "bdc2daa372a36f6f7c72abdc0b5639d1\n",
      "bdfb2c23a8c1dca5ea8c1cc3d89efee9\n",
      "be3e35bf8395366d235b8bcfc71a05ee\n",
      "be9a2df5a16434e581c6a0625c290591\n",
      "bf6a7a9ab4e18b18f43129c9e22fb448\n",
      "c0c5a155e6e59588783c2964975e7e1e\n",
      "c25876fb40d6f8dafd1ecb243193dd3f\n",
      "c2ef34cc347bc224b5a123426009d027\n",
      "c3a9046fbe2b0f0a4e43a669c321e472\n",
      "c46c3962c10e287f1c1e3af0d309a128\n",
      "c71d0db2086b7e2024ca9c11bd2ca504\n",
      "c7bdb83b7ca6269fac16ab7cff930a2e\n",
      "c87a713d17522698958de55c97654beb\n",
      "c95f2aa23e6d6702f5b16a3b35f89cf0\n",
      "cbb9bbd994c235b56fb77429291edf99\n",
      "cc1b7e34d9eba737c9fb91316463e8f7\n",
      "cc4805e3ebe8621bc94a621b1714fc84\n",
      "cd68d1a14cc504e3f7434d5cc324744d\n",
      "cd6be62834c72756738935f904ec9c2c\n",
      "cdb53f3be6d8cce07fa41c833488d8a5\n",
      "d03127f497cae40bcbd9996b4d1f5b90\n",
      "d032116d73789ff9c805f493357b4037\n",
      "d1131708024b32032ade1ef48d115915\n",
      "d1a20ef45bb03f93a407b492066f6d88\n",
      "d2ec8f0fc56a9168cda0c707e49974ab\n",
      "d3a8fb1da8f7a0dcbd5a8d65f3647757\n",
      "d42c998d037fb3003faba541e2cf649a\n",
      "d4a075768abe7fe43ad1caac92515256\n",
      "d5a0333be8795805fc39509f817780ee\n",
      "d654966fd2498de023552b830c07a659\n",
      "d753676c2c6c8ac6f97bd61ecab7554a\n",
      "d81852bffda09dc8033a45332397c495\n",
      "dbd9c8025907511e965e7abad955547d\n",
      "e0aa61b44c33e6a75940a8541c6894c9\n",
      "e314fd13809db0132443b924401d828b\n",
      "e33c25d0dbca5e54385f2100ce523467\n",
      "e3bc0a970a4af5d52826e06742f90e5b\n",
      "e42065c1145ccf734312cb9edbe5234b\n",
      "e60d99ea9648e1ce859eb0b386365e26\n",
      "e6160ed0ff2eb214abd4df9a3c336c1d\n",
      "e6d8ae8c3b0817df994a1ce3b37a7efb\n",
      "e9a27e2645e1fad9434ce765f678585f\n",
      "ea01deecde93cd9503a049d71d46e6d5\n",
      "ea3a771ef05e288409e0250ea893cf87\n",
      "eaeebb7a63edc8a329a7c5fbc583a507\n",
      "eb9db3f740f8e153e85f83c57bc4e522\n",
      "ebcdfabecf4b46b1e55e4a4c75a0afb0\n",
      "efcb6def7a2080243052b6046186ab24\n",
      "f0310ffc724faf9f7aef2c418127ee68\n",
      "f4d23e0272a2ce5bfc7f07033d4f2e7d\n",
      "f5ff7734997820b45dafa75dff60ece8\n",
      "f7c387290d7e3074501eac167c849000\n",
      "f89e3d0867e27be8e19d7ed50e1eb7e8\n",
      "fad57a1078ddbc685e517bd8f24aa8ac\n",
      "fb55849cee6473974612c17f094a38cd\n",
      "fb5874408966d7c6bebd3d84a5599e20\n",
      "fcfab3eddbdf0421c39f71d651cc5c56\n",
      "fdcd385b0d2d12341661e1abe845be0b\n",
      "ff8599dd7c1139be3bad5a0351ab749a\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import dicom\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "SLICE_COUNT = 20\n",
    "data_dir = 'D:/stage1/'\n",
    "model_save_folder = 'my-save-dir'\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "\n",
    "def PrintPrediction(patients, prediction, file):\n",
    "    with open(file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for i in range(len(patients)):\n",
    "            writer.writerow([patients[i], '0' if prediction[i][1] < 0 else\n",
    "                str(prediction[i][1])])\n",
    "\n",
    "def process_data(patient,img_px_size=50, hm_slices=20):\n",
    "    path = data_dir + patient\n",
    "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(np.array(each_slice.pixel_array),(img_px_size,img_px_size)) for each_slice in slices]\n",
    "    \n",
    "    chunk_sizes = math.ceil(len(slices) / hm_slices)\n",
    "    for slice_chunk in chunks(slices, chunk_sizes):\n",
    "        slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(slice_chunk)\n",
    "\n",
    "    if len(new_slices) == hm_slices-1:\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == hm_slices-2:\n",
    "        new_slices.append(new_slices[-1])\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == hm_slices+2:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "        \n",
    "    if len(new_slices) == hm_slices+1:\n",
    "        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n",
    "        del new_slices[hm_slices]\n",
    "        new_slices[hm_slices-1] = new_val\n",
    "        \n",
    "    return np.array(new_slices)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model_saver = tf.train.import_meta_graph(model_save_folder + '/my-model.meta')\n",
    "    model_saver.restore(sess, model_save_folder + '/my-model')   \n",
    "    x = tf.get_collection(\"input\")[0]\n",
    "    output = tf.get_collection(\"output\")[0]\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    #print(sess.run(tf.get_default_graph().get_tensor_by_name('w_conv1:0')))\n",
    "    \n",
    "    #collect list of preprocessed data on submission set\n",
    "    inputData = []\n",
    "    patients = []\n",
    "    isColumnHeader = True\n",
    "\n",
    "    with open('stage1_sample_submission.csv') as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for row in reader:\n",
    "            if not isColumnHeader:\n",
    "                patients.append(row[0])\n",
    "            isColumnHeader = False\n",
    "\n",
    "    for patient in patients:\n",
    "        #print(patient)\n",
    "        inputData.append(process_data(patient, img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT))\n",
    "\n",
    "    #prediction!\n",
    "    prediction = sess.run(output, feed_dict={x: inputData})\n",
    "    print(prediction)\n",
    "    PrintPrediction(patients, prediction, 'prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
